<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Event-based Video Super-Resolution">
  <meta property="og:title" content="Event-Enhanced Blurry Video Super-Resolution"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Video Super-Resolution, Event Camera, Video Deblur">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Event-Enhanced Blurry Video Super-Resolution</title>
  <link rel="icon" type="image/x-icon" href="static/images/EvDeblurVSR.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Event-Enhanced Blurry Video Super-Resolution</h1>

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/DachunKai" target="_blank">Dachun Kai<sup>1</sup></a>,</span>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=LatWlFAAAAAJ&hl=zh-CN&oi=ao" target="_blank">Yueyi Zhang<sup>† 1 2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/booker-max" target="_blank">Jin Wang<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://dblp.org/pid/276/3139.html" target="_blank">Zeyu Xiao<sup>3</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Snl0HPEAAAAJ&hl=zh-CN" target="_blank">Zhiwei Xiong<sup>1 2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=VRG3dw4AAAAJ&hl=zh-CN" target="_blank">Xiaoyan Sun<sup>1 2</sup></a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><small><sup>†</sup>Corresponding Author</small></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><small><sup>1</sup>University of Science and Technology of China</small></span>
              <span class="author-block"><small><sup>2</sup>Institute of Artificial Intelligence, Hefei Comprehensive National Science Center</small></span><br>
              <span class="author-block"><small><sup>3</sup>National University of Singapore</small></span>
            </div>                     

            <div class="is-size-5 publication-authors">
              <span class="author-block", style="color:#346dc2">AAAI 2025</span>
            </div>
            


                  <div class="column has-text-centered">
                    <!-- <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.13457" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32438" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/DachunKai/Ev-DeblurVSR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Poster link -->
                <!-- <span class="link-block">
                  <a href="https://docs.google.com/presentation/d/1nbDb39TFb374DzBwdz5v20kIREUA0nBH/edit?usp=sharing" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Video Demos</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/GOPR0384_11_00.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/GOPR0396_11_00.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/GOPR0410_11_00.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/GOPR0871_11_00.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we tackle the task of blurry video super-resolution (BVSR), aiming to generate high-resolution (HR) videos from low-resolution (LR) and blurry inputs. Current BVSR methods often fail to restore sharp details at high resolutions, resulting in noticeable artifacts and jitter due to insufficient motion information for deconvolution and the lack of high-frequency details in LR frames. To address these challenges, we introduce event signals into BVSR and propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse information from frames and events for feature deblurring, we introduce a reciprocal feature deblurring module that leverages motion information from intra-frame events to deblur frame features while reciprocally using global scene context from the frames to enhance event features. Furthermore, to enhance temporal consistency, we propose a hybrid deformable alignment module that fully exploits the complementary motion information from inter-frame events and optical flow to improve motion estimation in the deformable alignment process. Extensive evaluations demonstrate that Ev-DeblurVSR establishes a new state-of-the-art performance on both synthetic and real-world datasets. Notably, on real data, our method is +2.59dB more accurate and 7.28&times; faster than the recent best BVSR baseline FMA-Net.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Motivation -->
<!-- <section class="section">
  <div class="container"> -->
    <!-- Comparisons. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Motivation</h2>
        <image src="static/images/motivation.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
        <p>
        Comparative results of VSR methods on the City clip in <a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/sota/video-super-resolution-on-vid4-4x-upscaling">Vid4</a>. It can be observed that current VSR methods, with (<a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Learning_Spatial-Temporal_Implicit_Neural_Representations_for_Event-Guided_Video_Super-Resolution_CVPR_2023_paper.pdf">EGVSR</a> and <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/10222922">EBVSR</a>) or without (<a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.pdf">BasicVSR++</a>) event signals, still suffer from blurry textures or jitter effects, resulting in large errors in texture regions. In contrast, our method can predict the texture regions successfully and greatly reduce errors in the restored frames. 
        </p>
        </div>
      </div>
    </div>
</section> -->
<!-- End Motivation -->

<!-- Method Comparision -->
<!-- <section class="section">
  <div class="container"> -->
    <!-- Comparisons. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">VSR Methods Comparisons</h2>
        <image src="static/images/Comparison.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
        <p>
        <b>(a)</b> RGB-based methods usually focus on motion leaning to recover the missing details from other unaligned frames. <b>(b)</b> Previous event-based methods have attempted to use events to enhance the motion learning. <b>(c)</b> In contrast, our method is the first to utilize events to enhance the texture restoration in VSR. The <span style="color:red; font-weight:bold;">red</span> dotted line is an optional branch, where our method can easily adapt to approaches that use events to enhance the motion learning. 
        </p>
        </div>
      </div>
    </div>
</section> -->
<!-- End Method Comparision -->

<!-- Network Architecture -->
<section class="section">
  <div class="container">
    <!-- Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Network Architecture</h2>
        <image src="static/images/EvDeblurVSR_arch.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
        <p>
          Overview of Ev-DeblurVSR. Intra-frame voxels, which record the exposure time information of blurry frames, are fused with blurry frames in the RFD module to deblur frame features and enhance event features with scene context. Inter-frame voxels, which record motion information between frames, are integrated into the HDA module, using continuous motion trajectories to guide deformable alignment. Finally, the aligned features are upsampled to reconstruct sharp HR frames.
        </div>
      </div>
    </div>
</section>
<!-- End Network Architecture -->


<!-- Quantitative Results -->
<section class="section">
  <div class="container">
    <!-- Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Quantitative Results</h2>
        <image src="static/images/quantitative.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
          <p>
            Quantitative comparison (PSNR↑/SSIM↑/LPIPS↓) on the <a target="_blank" rel="noopener noreferrer" href="https://seungjunnah.github.io/Datasets/gopro.html">GoPro</a> dataset for 4× blurry VSR. More comparisons on <a target="_blank" rel="noopener noreferrer" href="https://github.com/zzh-tech/ESTRNN">BSD</a> and <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/view/neid2023">NCER</a> datasets are available in our paper.
          </p>          
        </div>
      </div>
    </div>
</section>
<!-- End Quantitative Results -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/quanlitative_gopro.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on <a target="_blank" rel="noopener noreferrer" href="https://seungjunnah.github.io/Datasets/gopro.html">GoPro</a> for 4&times; blurry VSR.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/quanlitative_bsd.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on <a target="_blank" rel="noopener noreferrer" href="https://github.com/zzh-tech/ESTRNN">BSD</a> for 4&times; blurry VSR.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/quanlitative_ncer.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/view/neid2023">NCER</a> for 4&times; blurry VSR. NCER is a real-world event dataset.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{kai2025event,
  title={Event-{E}nhanced {B}lurry {V}ideo {S}uper-{R}esolution},
  author={Kai, Dachun and Zhang, Yueyi and Wang, Jin and Xiao, Zeyu and Xiong, Zhiwei and Sun, Xiaoyan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={4},
  pages={4175--4183},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
