<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Event-based Video Super-Resolution">
  <meta property="og:title" content="EvTexture: Event-driven Texture Enhancement for Video Super-Resolution"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Video Super-Resolution, Event Camera, Texture Enhancement">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EvTexture: Event-driven Texture Enhancement for Video Super-Resolution</title>
  <link rel="icon" type="image/x-icon" href="static/images/EvTexture.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EvTexture: Event-driven Texture Enhancement for Video Super-Resolution</h1>

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/DachunKai" target="_blank">Dachun Kai<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="" target="_blank">Jiayao Lu<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=LatWlFAAAAAJ&hl=zh-CN&oi=ao" target="_blank">Yueyi Zhang<sup>† 1 2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=VRG3dw4AAAAJ&hl=zh-CN" target="_blank">Xiaoyan Sun<sup>1 2</sup></a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><small><sup>†</sup>Corresponding Author</small></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><small><sup>1</sup>University of Science and Technology of China</small></span>
              <span class="author-block"><small><sup>2</sup>Institute of Artificial Intelligence, Hefei Comprehensive National Science Center</small></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block", style="color:#346dc2">ICML 2024</span>
            </div>
            


                  <div class="column has-text-centered">
                    <!-- <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.13457" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2406.13457" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/DachunKai/EvTexture" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Poster link -->
                <span class="link-block">
                  <a href="https://docs.google.com/presentation/d/1nbDb39TFb374DzBwdz5v20kIREUA0nBH/edit?usp=sharing" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Video Demos</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/REDS_000.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/REDS_011.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/REDS_015.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/REDS_020.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Event-based vision has drawn increasing attention due to its unique characteristics, such as high temporal resolution and high dynamic range. It has been used in video super-resolution (VSR) recently to enhance the flow estimation and temporal alignment. Rather than for motion learning, we propose in this paper the first VSR method that utilizes event signals for texture enhancement. Our method, called EvTexture, leverages high-frequency details of events to better recover texture regions in VSR. In our EvTexture, a new texture enhancement branch is presented. We further introduce an iterative texture enhancement module to progressively explore the high-temporal-resolution event information for texture restoration. This allows for gradual refinement of texture regions across multiple iterations, leading to more accurate and rich high-resolut ion details. Experimental results show that our EvTexture achieves state-of-the-art performance on four datasets. For the Vid4 dataset with rich textures, our method can get up to 4.67dB gain compared with recent event-based methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Motivation -->
<section class="section">
  <div class="container">
    <!-- Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Motivation</h2>
        <image src="static/images/motivation.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
        <p>
        Comparative results of VSR methods on the City clip in <a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/sota/video-super-resolution-on-vid4-4x-upscaling">Vid4</a>. It can be observed that current VSR methods, with (<a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Learning_Spatial-Temporal_Implicit_Neural_Representations_for_Event-Guided_Video_Super-Resolution_CVPR_2023_paper.pdf">EGVSR</a> and <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/10222922">EBVSR</a>) or without (<a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.pdf">BasicVSR++</a>) event signals, still suffer from blurry textures or jitter effects, resulting in large errors in texture regions. In contrast, our method can predict the texture regions successfully and greatly reduce errors in the restored frames. 
        </p>
        </div>
      </div>
    </div>
</section>
<!-- End Motivation -->

<!-- Method Comparision -->
<section class="section">
  <div class="container">
    <!-- Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">VSR Methods Comparisons</h2>
        <image src="static/images/Comparison.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
        <p>
        <b>(a)</b> RGB-based methods usually focus on motion leaning to recover the missing details from other unaligned frames. <b>(b)</b> Previous event-based methods have attempted to use events to enhance the motion learning. <b>(c)</b> In contrast, our method is the first to utilize events to enhance the texture restoration in VSR. The <span style="color:red; font-weight:bold;">red</span> dotted line is an optional branch, where our method can easily adapt to approaches that use events to enhance the motion learning. 
        </p>
        </div>
      </div>
    </div>
</section>
<!-- End Method Comparision -->

<!-- Network Architecture -->
<section class="section">
  <div class="container">
    <!-- Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Network Architecture</h2>
        <image src="static/images/EvTexture_arch.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
        <p>
          <b>(a)</b> Following <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chan_BasicVSR_The_Search_for_Essential_Components_in_Video_Super-Resolution_and_CVPR_2021_paper.pdf">BasicVSR</a>, our EvTexture adopts a bidirectional recurrent network, where features are propagated forward and backward. At each timestamp, it includes a motion branch and a parallel texture branch to explicitly enhance the restoration of texture regions. <b>(b)</b> In the texture branch, the ITE module plays a key role. It progressively refines the feature across multiple iterations, leveraging high-frequency textural information from events along with context information from the current frame.
        </p>
        </div>
      </div>
    </div>
</section>
<!-- End Network Architecture -->


<!-- Quantitative Results -->
<section class="section">
  <div class="container">
    <!-- Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Quantitative Results</h2>
        <image src="static/images/quantitative.png" class="img-responsive" alt="comparison"><br>
        <div class="content has-text-justified">
        <p>
          Quantitative comparison (PSNR↑/SSIM↑) on <a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/sota/video-super-resolution-on-vid4-4x-upscaling">Vid4</a>, <a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/dataset/reds">REDS4</a> and <a target="_blank" rel="noopener noreferrer" href="http://toflow.csail.mit.edu/">Vimeo-90K-T</a> for 4&times; VSR. All results are calculated on Y-channel except REDS4 (RGB-channel). The input types "I" and "I+E" represent RGB-based and event-based methods, respectively. <span style="color:red; font-weight:bold;">Red</span> and <span style="color:blue; text-decoration:underline;">blue</span> colors indicate the best and second-best performances, respectively.
        </p>
        </div>
      </div>
    </div>
</section>
<!-- End Quantitative Results -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/quanlitative_vid4.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on <a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/sota/video-super-resolution-on-vid4-4x-upscaling">Vid4</a> for 4&times; VSR.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/quanlitative_vimeo.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on <a target="_blank" rel="noopener noreferrer" href="http://toflow.csail.mit.edu/">Vimeo-90K-T</a> for 4&times; VSR.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/quanlitative_reds.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison on <a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/dataset/reds">REDS4</a> for 4&times; VSR.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/quanlitative_ced.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Qualitative comparison on <a target="_blank" rel="noopener noreferrer" href="https://rpg.ifi.uzh.ch/CED.html">CED</a> for 4&times; VSR.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{kai2024evtexture,
  title={{E}v{T}exture: {E}vent-driven {T}exture {E}nhancement for {V}ideo {S}uper-{R}esolution},
  author={Kai, Dachun and Lu, Jiayao and Zhang, Yueyi and Sun, Xiaoyan},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={22817--22839},
  year={2024},
  volume={235},
  publisher={PMLR}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
